{"metadata":{"colab":{"provenance":[],"gpuType":"T4","machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"widgets":{"application/vnd.jupyter.widget-state+json":{"7e0e1984411446e09b3c5b3f547f840c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a1bb4f23ab044015853de3eeba196ac2","IPY_MODEL_057c720d54574483b68065447fa9a6b6","IPY_MODEL_c9102c35cb944ed8ad63b9f261e1a8b3"],"layout":"IPY_MODEL_aece9797c6d34af98dcea7e25c3d8a2e"}},"a1bb4f23ab044015853de3eeba196ac2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_81f19d0f2ffc4cb6acabc49bb66cee54","placeholder":"​","style":"IPY_MODEL_8b755297e0234c89b0a71efc927e1b75","value":"pytorch_model.bin: 100%"}},"057c720d54574483b68065447fa9a6b6":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_05e1b65695cd411ebf8b5f4a374176c3","max":558614189,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c14d927950324991ae4b21f4406d9b1b","value":558614189}},"c9102c35cb944ed8ad63b9f261e1a8b3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c6eb2127e8cd49369ab1bd5d90080709","placeholder":"​","style":"IPY_MODEL_c62d96e04d3d47858d2d27677b9df070","value":" 559M/559M [00:06&lt;00:00, 67.1MB/s]"}},"aece9797c6d34af98dcea7e25c3d8a2e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"81f19d0f2ffc4cb6acabc49bb66cee54":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8b755297e0234c89b0a71efc927e1b75":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"05e1b65695cd411ebf8b5f4a374176c3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c14d927950324991ae4b21f4406d9b1b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c6eb2127e8cd49369ab1bd5d90080709":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c62d96e04d3d47858d2d27677b9df070":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"accelerator":"GPU","kaggle":{"accelerator":"none","dataSources":[{"sourceId":7535479,"sourceType":"datasetVersion","datasetId":4388381}],"dockerImageVersionId":30646,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"! pip install sentencepiece","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9e3Gqb3Ni2lQ","outputId":"6a95383d-297c-47ce-e495-34b0c360135f","execution":{"iopub.status.busy":"2024-02-13T06:06:43.349357Z","iopub.execute_input":"2024-02-13T06:06:43.349730Z","iopub.status.idle":"2024-02-13T06:06:55.510757Z","shell.execute_reply.started":"2024-02-13T06:06:43.349698Z","shell.execute_reply":"2024-02-13T06:06:55.509550Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (0.1.99)\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport json\nfrom torch.utils.data import Dataset, DataLoader\n#from transformers import DebertaV2Tokenizer as tkz\nfrom transformers import DebertaV2ForMultipleChoice as mpc\nfrom transformers import DebertaTokenizer as tkz\n#from transformers import DebertaForSequenceClassification as mpc\nimport torch.optim as optim","metadata":{"id":"rGaK1DJl5XsS","execution":{"iopub.status.busy":"2024-02-13T06:07:27.792882Z","iopub.execute_input":"2024-02-13T06:07:27.794170Z","iopub.status.idle":"2024-02-13T06:07:27.800198Z","shell.execute_reply.started":"2024-02-13T06:07:27.794110Z","shell.execute_reply":"2024-02-13T06:07:27.799254Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Reading the train dataset back\nwith open('/kaggle/input/cf-dataset/train_dataset_final.json', 'r') as train_file:\n    train_dataset_back = json.load(train_file)\n\n# Reading the test dataset back\nwith open('/kaggle/input/cf-dataset/test_dataset_final.json', 'r') as test_file:\n    test_dataset_back = json.load(test_file)\n\n# Display the sizes of the two datasets\nprint(f\"Size of the train dataset: {len(train_dataset_back)}\")\nprint(f\"Size of the test dataset: {len(test_dataset_back)}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iRh-Kb8WEw78","outputId":"cb1abcf2-5786-466e-dd15-ce4fb0fc82f8","execution":{"iopub.status.busy":"2024-02-13T06:07:31.773883Z","iopub.execute_input":"2024-02-13T06:07:31.774586Z","iopub.status.idle":"2024-02-13T06:07:31.796640Z","shell.execute_reply.started":"2024-02-13T06:07:31.774554Z","shell.execute_reply":"2024-02-13T06:07:31.795757Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Size of the train dataset: 1338\nSize of the test dataset: 599\n","output_type":"stream"}]},{"cell_type":"code","source":"class TTDataset(Dataset):\n    def __init__(self, copa_data, tokenizer):\n        self.copa_data = copa_data\n        self.tokenizer = tokenizer\n\n    def __len__(self):\n        return len(self.copa_data)\n\n    def __getitem__(self, idx):\n        data = self.copa_data[idx]\n        premise = data['premise']\n        hypotheses = [data['initial'], data['counterfactual']]\n        label = data['label']\n\n        # Tokenize\n        encoded_input = self.tokenizer([premise] * 2, hypotheses, padding='max_length', truncation=True, return_tensors='pt')\n        encoded_input['labels'] = torch.tensor(label)\n\n        return encoded_input\n","metadata":{"id":"Krfexz-48T6w","execution":{"iopub.status.busy":"2024-02-13T06:07:34.913556Z","iopub.execute_input":"2024-02-13T06:07:34.914327Z","iopub.status.idle":"2024-02-13T06:07:34.921189Z","shell.execute_reply.started":"2024-02-13T06:07:34.914295Z","shell.execute_reply":"2024-02-13T06:07:34.920177Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"model_base = 'microsoft/deberta-base'\n\n# Load tokenizer and model\ntokenizer = tkz.from_pretrained(model_base)\nmodel = mpc.from_pretrained(model_base)\n\n# Prepare the DataLoader\nTT_dataset = TTDataset(train_dataset_back, tokenizer)\ndataloader = DataLoader(TT_dataset, batch_size=4, shuffle=True)\n\n# Optimizer and Loss Function\noptimizer = optim.Adam(model.parameters(), lr=1e-5)\nloss_fn = torch.nn.CrossEntropyLoss()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":123,"referenced_widgets":["7e0e1984411446e09b3c5b3f547f840c","a1bb4f23ab044015853de3eeba196ac2","057c720d54574483b68065447fa9a6b6","c9102c35cb944ed8ad63b9f261e1a8b3","aece9797c6d34af98dcea7e25c3d8a2e","81f19d0f2ffc4cb6acabc49bb66cee54","8b755297e0234c89b0a71efc927e1b75","05e1b65695cd411ebf8b5f4a374176c3","c14d927950324991ae4b21f4406d9b1b","c6eb2127e8cd49369ab1bd5d90080709","c62d96e04d3d47858d2d27677b9df070"]},"id":"FptGJyplT5lH","outputId":"66159da6-6146-44f4-d730-8f37b4d6b0c2","execution":{"iopub.status.busy":"2024-02-13T06:07:40.265767Z","iopub.execute_input":"2024-02-13T06:07:40.266349Z","iopub.status.idle":"2024-02-13T06:07:41.218671Z","shell.execute_reply.started":"2024-02-13T06:07:40.266307Z","shell.execute_reply":"2024-02-13T06:07:41.217727Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"You are using a model of type deberta to instantiate a model of type deberta-v2. This is not supported for all configurations of models and can yield errors.\n/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\nSome weights of DebertaV2ForMultipleChoice were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'deberta.encoder.layer.0.attention.self.key_proj.bias', 'deberta.encoder.layer.0.attention.self.key_proj.weight', 'deberta.encoder.layer.0.attention.self.pos_key_proj.bias', 'deberta.encoder.layer.0.attention.self.pos_key_proj.weight', 'deberta.encoder.layer.0.attention.self.pos_query_proj.bias', 'deberta.encoder.layer.0.attention.self.pos_query_proj.weight', 'deberta.encoder.layer.0.attention.self.query_proj.bias', 'deberta.encoder.layer.0.attention.self.query_proj.weight', 'deberta.encoder.layer.0.attention.self.value_proj.bias', 'deberta.encoder.layer.0.attention.self.value_proj.weight', 'deberta.encoder.layer.1.attention.self.key_proj.bias', 'deberta.encoder.layer.1.attention.self.key_proj.weight', 'deberta.encoder.layer.1.attention.self.pos_key_proj.bias', 'deberta.encoder.layer.1.attention.self.pos_key_proj.weight', 'deberta.encoder.layer.1.attention.self.pos_query_proj.bias', 'deberta.encoder.layer.1.attention.self.pos_query_proj.weight', 'deberta.encoder.layer.1.attention.self.query_proj.bias', 'deberta.encoder.layer.1.attention.self.query_proj.weight', 'deberta.encoder.layer.1.attention.self.value_proj.bias', 'deberta.encoder.layer.1.attention.self.value_proj.weight', 'deberta.encoder.layer.10.attention.self.key_proj.bias', 'deberta.encoder.layer.10.attention.self.key_proj.weight', 'deberta.encoder.layer.10.attention.self.pos_key_proj.bias', 'deberta.encoder.layer.10.attention.self.pos_key_proj.weight', 'deberta.encoder.layer.10.attention.self.pos_query_proj.bias', 'deberta.encoder.layer.10.attention.self.pos_query_proj.weight', 'deberta.encoder.layer.10.attention.self.query_proj.bias', 'deberta.encoder.layer.10.attention.self.query_proj.weight', 'deberta.encoder.layer.10.attention.self.value_proj.bias', 'deberta.encoder.layer.10.attention.self.value_proj.weight', 'deberta.encoder.layer.11.attention.self.key_proj.bias', 'deberta.encoder.layer.11.attention.self.key_proj.weight', 'deberta.encoder.layer.11.attention.self.pos_key_proj.bias', 'deberta.encoder.layer.11.attention.self.pos_key_proj.weight', 'deberta.encoder.layer.11.attention.self.pos_query_proj.bias', 'deberta.encoder.layer.11.attention.self.pos_query_proj.weight', 'deberta.encoder.layer.11.attention.self.query_proj.bias', 'deberta.encoder.layer.11.attention.self.query_proj.weight', 'deberta.encoder.layer.11.attention.self.value_proj.bias', 'deberta.encoder.layer.11.attention.self.value_proj.weight', 'deberta.encoder.layer.2.attention.self.key_proj.bias', 'deberta.encoder.layer.2.attention.self.key_proj.weight', 'deberta.encoder.layer.2.attention.self.pos_key_proj.bias', 'deberta.encoder.layer.2.attention.self.pos_key_proj.weight', 'deberta.encoder.layer.2.attention.self.pos_query_proj.bias', 'deberta.encoder.layer.2.attention.self.pos_query_proj.weight', 'deberta.encoder.layer.2.attention.self.query_proj.bias', 'deberta.encoder.layer.2.attention.self.query_proj.weight', 'deberta.encoder.layer.2.attention.self.value_proj.bias', 'deberta.encoder.layer.2.attention.self.value_proj.weight', 'deberta.encoder.layer.3.attention.self.key_proj.bias', 'deberta.encoder.layer.3.attention.self.key_proj.weight', 'deberta.encoder.layer.3.attention.self.pos_key_proj.bias', 'deberta.encoder.layer.3.attention.self.pos_key_proj.weight', 'deberta.encoder.layer.3.attention.self.pos_query_proj.bias', 'deberta.encoder.layer.3.attention.self.pos_query_proj.weight', 'deberta.encoder.layer.3.attention.self.query_proj.bias', 'deberta.encoder.layer.3.attention.self.query_proj.weight', 'deberta.encoder.layer.3.attention.self.value_proj.bias', 'deberta.encoder.layer.3.attention.self.value_proj.weight', 'deberta.encoder.layer.4.attention.self.key_proj.bias', 'deberta.encoder.layer.4.attention.self.key_proj.weight', 'deberta.encoder.layer.4.attention.self.pos_key_proj.bias', 'deberta.encoder.layer.4.attention.self.pos_key_proj.weight', 'deberta.encoder.layer.4.attention.self.pos_query_proj.bias', 'deberta.encoder.layer.4.attention.self.pos_query_proj.weight', 'deberta.encoder.layer.4.attention.self.query_proj.bias', 'deberta.encoder.layer.4.attention.self.query_proj.weight', 'deberta.encoder.layer.4.attention.self.value_proj.bias', 'deberta.encoder.layer.4.attention.self.value_proj.weight', 'deberta.encoder.layer.5.attention.self.key_proj.bias', 'deberta.encoder.layer.5.attention.self.key_proj.weight', 'deberta.encoder.layer.5.attention.self.pos_key_proj.bias', 'deberta.encoder.layer.5.attention.self.pos_key_proj.weight', 'deberta.encoder.layer.5.attention.self.pos_query_proj.bias', 'deberta.encoder.layer.5.attention.self.pos_query_proj.weight', 'deberta.encoder.layer.5.attention.self.query_proj.bias', 'deberta.encoder.layer.5.attention.self.query_proj.weight', 'deberta.encoder.layer.5.attention.self.value_proj.bias', 'deberta.encoder.layer.5.attention.self.value_proj.weight', 'deberta.encoder.layer.6.attention.self.key_proj.bias', 'deberta.encoder.layer.6.attention.self.key_proj.weight', 'deberta.encoder.layer.6.attention.self.pos_key_proj.bias', 'deberta.encoder.layer.6.attention.self.pos_key_proj.weight', 'deberta.encoder.layer.6.attention.self.pos_query_proj.bias', 'deberta.encoder.layer.6.attention.self.pos_query_proj.weight', 'deberta.encoder.layer.6.attention.self.query_proj.bias', 'deberta.encoder.layer.6.attention.self.query_proj.weight', 'deberta.encoder.layer.6.attention.self.value_proj.bias', 'deberta.encoder.layer.6.attention.self.value_proj.weight', 'deberta.encoder.layer.7.attention.self.key_proj.bias', 'deberta.encoder.layer.7.attention.self.key_proj.weight', 'deberta.encoder.layer.7.attention.self.pos_key_proj.bias', 'deberta.encoder.layer.7.attention.self.pos_key_proj.weight', 'deberta.encoder.layer.7.attention.self.pos_query_proj.bias', 'deberta.encoder.layer.7.attention.self.pos_query_proj.weight', 'deberta.encoder.layer.7.attention.self.query_proj.bias', 'deberta.encoder.layer.7.attention.self.query_proj.weight', 'deberta.encoder.layer.7.attention.self.value_proj.bias', 'deberta.encoder.layer.7.attention.self.value_proj.weight', 'deberta.encoder.layer.8.attention.self.key_proj.bias', 'deberta.encoder.layer.8.attention.self.key_proj.weight', 'deberta.encoder.layer.8.attention.self.pos_key_proj.bias', 'deberta.encoder.layer.8.attention.self.pos_key_proj.weight', 'deberta.encoder.layer.8.attention.self.pos_query_proj.bias', 'deberta.encoder.layer.8.attention.self.pos_query_proj.weight', 'deberta.encoder.layer.8.attention.self.query_proj.bias', 'deberta.encoder.layer.8.attention.self.query_proj.weight', 'deberta.encoder.layer.8.attention.self.value_proj.bias', 'deberta.encoder.layer.8.attention.self.value_proj.weight', 'deberta.encoder.layer.9.attention.self.key_proj.bias', 'deberta.encoder.layer.9.attention.self.key_proj.weight', 'deberta.encoder.layer.9.attention.self.pos_key_proj.bias', 'deberta.encoder.layer.9.attention.self.pos_key_proj.weight', 'deberta.encoder.layer.9.attention.self.pos_query_proj.bias', 'deberta.encoder.layer.9.attention.self.pos_query_proj.weight', 'deberta.encoder.layer.9.attention.self.query_proj.bias', 'deberta.encoder.layer.9.attention.self.query_proj.weight', 'deberta.encoder.layer.9.attention.self.value_proj.bias', 'deberta.encoder.layer.9.attention.self.value_proj.weight', 'pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"from psutil import virtual_memory\nram_gb = virtual_memory().total / 1e9\nprint('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n\nif ram_gb < 20:\n  print('Not using a high-RAM runtime')\nelse:\n  print('You are using a high-RAM runtime!')","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IkMBpabd-zl5","outputId":"a96a7e43-ae9f-4ba2-add0-a1a258bf100e","execution":{"iopub.status.busy":"2024-02-13T06:07:50.010095Z","iopub.execute_input":"2024-02-13T06:07:50.010788Z","iopub.status.idle":"2024-02-13T06:07:50.019474Z","shell.execute_reply.started":"2024-02-13T06:07:50.010753Z","shell.execute_reply":"2024-02-13T06:07:50.018257Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Your runtime has 33.7 gigabytes of available RAM\n\nYou are using a high-RAM runtime!\n","output_type":"stream"}]},{"cell_type":"code","source":"# Training Loop\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KoKo_ScLdXkC","outputId":"8991a0fb-d126-435c-dc57-3ca479ce8031","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Training Loop\nc=0\nfor epoch in range(3):  # Number of epochs\n    model.train()\n    for batch in dataloader:\n        optimizer.zero_grad()\n\n        input_ids = batch['input_ids'].squeeze(0)\n        attention_mask = batch['attention_mask'].squeeze(0)\n        labels = batch['labels'].squeeze(0)\n\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n        loss = outputs.loss\n\n        loss.backward()\n        optimizer.step()\n        c+=1\n\n        print(f\"count:{c},Epoch: {epoch}, Loss: {loss.item()}\")\n\nprint('Training done')","metadata":{"id":"mF37lYS5F6OG","execution":{"iopub.status.busy":"2024-02-13T12:35:07.919966Z","iopub.execute_input":"2024-02-13T12:35:07.920677Z","iopub.status.idle":"2024-02-13T12:35:08.376363Z","shell.execute_reply.started":"2024-02-13T12:35:07.920622Z","shell.execute_reply":"2024-02-13T12:35:08.374949Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":1,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m c\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m3\u001b[39m):  \u001b[38;5;66;03m# Number of epochs\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[1;32m      6\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n","\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"],"ename":"NameError","evalue":"name 'model' is not defined","output_type":"error"}]},{"cell_type":"code","source":"#tokenizer = tkz.from_pretrained(model_base)\nvalidation_dataset = TTDataset(test_dataset_back, tokenizer)\nvalidation_dataloader = DataLoader(validation_dataset, batch_size=8)","metadata":{"id":"VzloT11bom67","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\nmodel.eval()\ncorrect_predictions = 0\ntotal_predictions = 0\n\naccuracy, f1, precision, recall = [], [], [], []\nbatches = 0\n\ndef compute_metrics(y_true, y_pred):\n    accuracy = accuracy_score(y_true, y_pred)\n    f1 = f1_score(y_true, y_pred)\n    precision = precision_score(y_true, y_pred)\n    recall = recall_score(y_true, y_pred)\n    return [accuracy, f1, precision, recall]\n\nwith torch.no_grad():\n    for batch in validation_dataloader:\n        input_ids = batch['input_ids'].squeeze(1)\n        attention_mask = batch['attention_mask'].squeeze(1)\n        labels = batch['labels']  # ground truth\n\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n        predictions = torch.argmax(outputs.logits, dim=1)  # predictions\n\n        calculated_metrics = compute_metrics(labels, predictions)\n        accuracy.append(calculated_metrics[0])\n        f1.append(calculated_metrics[1])\n        precision.append(calculated_metrics[2])\n        recall.append(calculated_metrics[3])\n        batches += 1\n\n        correct_predictions += (predictions == labels).sum().item()\n        total_predictions += labels.size(0)\n\noverall_accuracy = correct_predictions / total_predictions\nprint(f\"Overall Accuracy: {overall_accuracy:.4f}\")\n\nprint(f\"Accuracy: {sum(accuracy)/batches:.4f}\")\nprint(f\"F1: {sum(f1)/batches:.4f}\")\nprint(f\"Precision: {sum(precision)/batches:.4f}\")\nprint(f\"Recall: {sum(recall)/batches:.4f}\")\n\nprint(f'Number of batches: {batches}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.eval()\ncorrect_predictions = 0\ntotal_predictions = 0\n\nwith torch.no_grad():\n    for batch in validation_dataloader:\n        input_ids = batch['input_ids'].squeeze(1)\n        attention_mask = batch['attention_mask'].squeeze(1)\n        labels = batch['labels']\n\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n        predictions = torch.argmax(outputs.logits, dim=1)\n        correct_predictions += (predictions == labels).sum().item()\n        total_predictions += labels.size(0)\n\noverall_accuracy = correct_predictions / total_predictions\nprint(f\"Overall Accuracy: {overall_accuracy:.4f}\")","metadata":{"id":"ZJF2USWQozNb","colab":{"base_uri":"https://localhost:8080/"},"outputId":"b3b43727-1c79-4b75-a489-90b28dad5e35"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Specify the path where you want to save the model\nmodel_path = '/content/model_DeBERTA.pth'\n\n# Save the entire model\ntorch.save(model, model_path)\n","metadata":{"id":"j70V1ruI71Zg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_sd_path = '/content/model_DeBERTA_sd.pth'\ntorch.save(model.state_dict(), model_sd_path)","metadata":{"id":"33P8bqqi8pQM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = mpc.from_pretrained(model_base)\nmodel.load_state_dict(torch.load(model_sd_path))","metadata":{"id":"BLu20uC1JW1M","colab":{"base_uri":"https://localhost:8080/"},"outputId":"f1820066-067a-4845-f0c7-5ee0cd0f6cdf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define split sizes for a 75-25 split\nval_dataset_size = int(0.10 * len(test_dataset_back))\n\nval_data = test_dataset_back[:val_dataset_size]\nlen(val_data)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3ewCKlNg9-Rx","outputId":"d1dd3325-05a3-428d-88bb-b14d7977f750"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = tkz.from_pretrained(model_base)\nval_dataset = TTDataset(val_data, tokenizer)\nval_dataloader = DataLoader(val_dataset, batch_size=8)","metadata":{"id":"IDRU5QcB9sUV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.eval()\ncorrect_predictions = 0\ntotal_predictions = 0\n\nwith torch.no_grad():\n    for batch in val_dataloader:\n        input_ids = batch['input_ids'].squeeze(1)\n        attention_mask = batch['attention_mask'].squeeze(1)\n        labels = batch['labels']\n\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n        predictions = torch.argmax(outputs.logits, dim=1)\n        correct_predictions += (predictions == labels).sum().item()\n        total_predictions += labels.size(0)\n\noverall_accuracy = correct_predictions / total_predictions\nprint(f\"Overall Accuracy: {overall_accuracy:.4f}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LWm5s8C1-tz8","outputId":"d0dfa5f0-1c3b-47bd-b9d9-3d664c13dfd3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Neuro Symbolic test","metadata":{"id":"s0Wq3u8rVW3t"}},{"cell_type":"code","source":"class COPADataset(Dataset):\n    def __init__(self, dataframe, tokenizer):\n        self.dataframe = dataframe\n        self.tokenizer = tokenizer\n\n    def __len__(self):\n        return len(self.dataframe)\n\n    def __getitem__(self, idx):\n        try:\n            # Extracting data from the dataframe\n            data = self.dataframe.iloc[idx]\n            premise = data['premise']\n            hypothesis1 = data['hypothesis1']\n            hypothesis2 = data['hypothesis2']\n            label = data['label']\n\n            # Concatenating the premise and hypotheses with their respective relations\n            premise_with_relations = premise + \" \" + data['premise_relations']\n            hypothesis1_with_relations = hypothesis1 + \" \" + data['hypothesis1_relations']\n            hypothesis2_with_relations = hypothesis2 + \" \" + data['hypothesis2_relations']\n\n            # Tokenize\n            encoded_input = self.tokenizer(\n                [premise_with_relations] * 2,\n                [hypothesis1_with_relations, hypothesis2_with_relations],\n                padding='max_length',\n                truncation=True,\n                max_length=512,  # Or another suitable max length\n                return_tensors='pt'\n            )\n            encoded_input['labels'] = torch.tensor(label)\n\n            return encoded_input\n\n        except KeyError as e:\n            print(f\"KeyError encountered at index {idx}: {e}\")\n            raise\n","metadata":{"id":"AiuWTvLqCLQx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom transformers import BertTokenizer, BertForMultipleChoice\nimport torch\nfrom torch.utils.data import DataLoader\nimport torch.optim as optim\n\nprocessed_data = pd.read_csv('/content/processed_train_data.csv')\n# Load tokenizer and model\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel = BertForMultipleChoice.from_pretrained('bert-base-uncased')\n\n# Prepare the DataLoader\n# Convert your dataframe to the updated dataset\ntrain_dataset = COPADataset(processed_data, tokenizer)\ntrain_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n\n\n# Optimizer and Loss Function\noptimizer = optim.Adam(model.parameters(), lr=1e-5)\nloss_fn = torch.nn.CrossEntropyLoss()\n\n# Training Loop\nmodel.train()\nfor epoch in range(3):  # Number of epochs\n    for batch in train_dataloader:\n        optimizer.zero_grad()\n\n        input_ids = batch['input_ids'].squeeze(1)\n        attention_mask = batch['attention_mask'].squeeze(1)\n        labels = batch['labels']\n\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n        loss = outputs.loss\n\n        loss.backward()\n        optimizer.step()\n\n        print(f\"Epoch: {epoch}, Loss: {loss.item()}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"45Y1OrB2-E46","outputId":"6ea2b99b-1b03-4ea0-948a-ca5399817a71"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_data = pd.read_csv('/content/processed_validation_data.csv')\n# Load tokenizer and model\n\nvalidation_dataset = COPADataset(val_data, tokenizer)\nvalidation_dataloader = DataLoader(validation_dataset, batch_size=8)\n","metadata":{"id":"iZ5-qIr6_107"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.eval()\ncorrect_predictions = 0\ntotal_predictions = 0\n\nwith torch.no_grad():\n    for batch in validation_dataloader:\n        input_ids = batch['input_ids'].squeeze(1)\n        attention_mask = batch['attention_mask'].squeeze(1)\n        labels = batch['labels']\n\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n        predictions = torch.argmax(outputs.logits, dim=1)\n        correct_predictions += (predictions == labels).sum().item()\n        total_predictions += labels.size(0)\n\noverall_accuracy = correct_predictions / total_predictions\nprint(f\"Overall Accuracy: {overall_accuracy:.4f}\")\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Jy2xVur-VkWi","outputId":"9331e1bf-40a7-4fa6-ab30-54d65df7641d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"IpR-EQgBWMNQ"},"execution_count":null,"outputs":[]}]}